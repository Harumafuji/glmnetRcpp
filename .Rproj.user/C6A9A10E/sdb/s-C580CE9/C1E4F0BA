{
    "collab_server" : "",
    "contents" : "rm(list = ls())\nlibrary(h2o)\nlibrary(glmnetRcpp)\n\n# negative log likelihood of exponential distribution\n\nExpNegativeLogLikelihood = function(x, params) {\n        rs = params[[\"A\"]] %*% x\n        return(t(params[[\"b\"]]) %*% exp(-rs) + sum(rs))\n}\n\n# gradient of the negative log likelihood of exponential distribution\n\nGradExpNegativeLogLikelihood = function(x, params) {\n        tmp = -t(params[[\"A\"]]) %*% (exp(-params[[\"A\"]] %*% x) * params[[\"b\"]] - 1)\n        return(tmp)\n}\n\n# proximal of L1 regularizer\nprox_L1 = function(x, threshold) {\n        return(sapply(x, function(x)\n                sign(x) * max(abs(x) - threshold, 0)))\n}\n\n# L1 regularizer\nregularizer_L1 = function(x) {\n        return(sum(abs(x)))\n}\n\n# Elastic Net regularizer\nregularizer_ENet = function(x, alpha = 1){\n        return(alpha * regularizer_L1(x) + (1 - alpha) / 2 * sum(x^2))\n}\n\n# proximal of Elastic Net regularizer, not run\n# prox_ENet = function(x, threshold, alpha = 1){\n#         return( alpha * t / (1 + t * ( 1 - alpha )))\n# }\n\n# complete objective function\nObjFun = function(x,\n                  params,\n                  SmoothFun = ExpNegativeLogLikelihood,\n                  regularizer = regularizer_ENet) {\n        return(SmoothFun(x, params) +\n                       params[[\"lambda\"]] * regularizer(x, params[[\"alpha\"]]))\n}\n\n# predict output given the coefficients\npredict_exp_glmnet = function(x, params) {\n        return(exp(params[[\"A\"]] %*% x))\n}\n\n# fit glmnet with vanilla pgd\n\nfit_glmnet_fixed_lambda_vanilla_pgd = function(A,\n                                               b,\n                                               x0 = rep(0, ncol(A)),\n                                               alpha = 0.5,\n                                               lambda = 0,\n                                               maxiter = 10000,\n                                               ABSTOL = 1e-5,\n                                               RELTOL = 1e-2,\n                                               t = 1,\n                                               beta = 1 / 2) {\n        params = list(\n                b = b,\n                # The response variables\n                A = A,\n                # The matrix\n                alpha = alpha,\n                # 1 means lasso\n                lambda = lambda\n        ) # the regularization coefficient\n        prox.lambda = 1\n        beta = beta\n        max_iter = maxiter\n        ABSTOL   = ABSTOL\n\n        # save algorithm state\n        nvars = ncol(A)\n        x_matrix = matrix(0, nrow = max_iter, ncol = nvars)\n        grad_x_matrix = matrix(0, nrow = max_iter, ncol = nvars)\n        z_matrix = matrix(0, nrow = max_iter, ncol = nvars)\n        lhs_vector = rep(NA, max_iter)\n        rhs_vector = rep(NA, max_iter)\n        obj_vector = rep(NA, max_iter)\n\n        x = x0\n        xprev = x\n\n        for (k in 1:max_iter) {\n                while (TRUE) {\n                        grad_x = GradExpNegativeLogLikelihood(x, params)\n                        threshold = prox.lambda * params[[\"lambda\"]] * params[[\"alpha\"]] /\n                                (1+prox.lambda * params[[\"lambda\"]]*(1-params[[\"alpha\"]]))\n                        z = prox_L1(x - prox.lambda * grad_x, threshold)\n                        lhs = ExpNegativeLogLikelihood(z, params)\n                        rhs = ExpNegativeLogLikelihood(x, params)\n                        rhs = rhs + t(grad_x) %*% (z - x)\n                        rhs = rhs + (1 / (2 * prox.lambda)) * sum((z - x) ^ 2)\n                        if (lhs <= rhs) {\n                                break\n                        }\n                        prox.lambda = beta * prox.lambda\n                }\n\n                xprev = x\n                x = z\n                x_matrix[k, ] = xprev\n                z_matrix[k, ] = z\n                grad_x_matrix[k, ] = grad_x\n                lhs_vector[k] = lhs\n                rhs_vector[k] = rhs\n\n                obj_vector[k] = ObjFun(x, params)\n                if (k > 1)\n                        if (abs(obj_vector[k] - obj_vector[k - 1]) < ABSTOL) {\n                                break\n                        }\n        }\n        x_matrix = x_matrix[1:k, ]\n        z_matrix = z_matrix[1:k, ]\n        grad_x_matrix = grad_x_matrix[1:k, ]\n        lhs_vector = lhs_vector[1:k]\n        rhs_vector = rhs_vector[1:k]\n\n        obj_vector\n        grad_x_matrix\n        lhs_vector\n        rhs_vector\n        z_matrix\n        x.true\n        return(z_matrix[k,])\n}\n\nfit_glmnet_fixed_lambda_vanilla_pgd_cv = function(A,\n                                           b,\n                                           lambda,\n                                           x0 = rep(0, ncol(A)),\n                                           alpha = 0.5,\n                                           maxiter = 1000,\n                                           ABSTOL = 1e-4,\n                                           RELTOL = 1e-2,\n                                           t = 1,\n                                           beta = 1/2,\n                                           kfold = 5){\n\n        res = list()\n        for(ii in 1:kfold){\n                train.idx = sample(nrow(A), floor((1 - 1/kfold) * nrow(A)))\n                A.train = A[train.idx,]\n                b.train = b[train.idx]\n                A.test = A[-train.idx,]\n                b.test = b[-train.idx]\n                x.pred = fit_glmnet_fixed_lambda_vanilla_pgd(A.train,\n                                                      b.train,\n                                                      x0 = x0,\n                                                      alpha = alpha,\n                                                      lambda = lambda,\n                                                      maxiter = maxiter,\n                                                      ABSTOL = ABSTOL,\n                                                      RELTOL = RELTOL,\n                                                      t = t,\n                                                      beta = beta)\n                params = list(\n                        b = b.test,\n                        # The response variables\n                        A = A.test,\n                        # The matrix\n                        alpha = alpha,\n                        # 1 means lasso\n                        lambda = lambda\n                ) # the regularization coefficient\n                b.pred = predict_exp_glmnet(x.pred, params)\n                rse = sqrt(sum((b.pred - b.test)^2))\n                res[[ii]] = list(x.pred = x.pred, A.test = A.test, b.test = b.test,\n                                 b.pred = b.pred, rse = rse,\n                                 A.train = A.train, b.train = b.train)\n        }\n        rmse = sapply(res, function(x) x$rse)\n        rmse = mean(rmse)\n        return(list(rmse = rmse, raw.res = res))\n}\n\n# fit glm-en with vanillda PGD, use k-fold cv to estimate the prediction error and automatically find optimal lambda\n\nfit_glmnet_search_lambda_vanilla_pgd_cv = function(A,\n                                            b,\n                                            n.lambda = 100,\n                                            min.lambda.ratio = 10e-4,\n                                            x0 = rep(0, ncol(A)),\n                                            alpha = 0.5,\n                                            maxiter = 1000,\n                                            ABSTOL = 1e-4,\n                                            RELTOL = 1e-2,\n                                            t = 1,\n                                            beta = 1/2,\n                                            kfold = 5){\n        lambda_max = find_lambda_max(A, b, alpha)\n        lambdas = 10^(seq(log10(lambda_max * min.lambda.ratio), log10(lambda_max), length.out = n.lambda))\n        res = list()\n        rmse_vector = rep(0, n.lambda)\n        for(ii in 1:n.lambda){\n                res[[ii]] = fit_glmnet_fixed_lambda_vanilla_pgd_cv(A,\n                                                            b,\n                                                            lambda = lambdas[ii],\n                                                            x0 = x0,\n                                                            alpha = alpha,\n                                                            maxiter = maxiter,\n                                                            ABSTOL = ABSTOL,\n                                                            RELTOL = RELTOL,\n                                                            t = t,\n                                                            beta = beta,\n                                                            kfold = kfold)\n                rmse_vector[ii] = res[[ii]]$rmse\n        }\n        best.idx = which.min(rmse_vector)\n        best.lambda = lambdas[best.idx]\n        best.x = fit_glmnet_fixed_lambda_vanilla_pgd(A,\n                                              b,\n                                              x0 = x0,\n                                              alpha = alpha,\n                                              lambda = best.lambda,\n                                              maxiter = maxiter,\n                                              ABSTOL = ABSTOL,\n                                              RELTOL = RELTOL,\n                                              t = t,\n                                              beta = beta)\n        return(list(best.x = best.x,\n                    best.idx = best.idx,\n                    best.lambda = best.lambda,\n                    rmse = rmse_vector,\n                    cv.res = res\n        ))\n}\n\n\n\n\nfind_lambda_max = function(A,\n                           b,\n                           alpha){\n        return(max(abs((b-1)%*%A))/alpha)\n}\n\n\n# find the smallest enet_lambda that garuantees zero solution\n\nfind_lambda_max = function(A,\n                           b,\n                           alpha,\n                           scale = 10){\n        return(max(abs((b-1)%*%A))/alpha/scale)\n}\n\n# generate vector of candidate lambdas\n\ngenerate_lambda_grid = function(A, b, alpha,\n                                n.lambda = 100,\n                                min.lambda.ratio = 1e-4\n                                ){\n        lambda_max = find_lambda_max(A, b, alpha)\n        lambdas = 10^(seq(log10(lambda_max * min.lambda.ratio), log10(lambda_max), length.out = n.lambda))\n}\n\n\n\n######### generate test data\nset.seed(20170820)\n\nnobs = 50\nnvars = 7\n\nx.true = rnorm(nvars)\nx.true[sample(2:nvars, floor(nvars / 2) - 1)] = 0\n\n## random normal matrix A\nA = matrix(rnorm(nvars * nobs), ncol = nvars)\n\n# ## nobs same row\n# A = matrix(rnorm(nvars), ncol = nvars, nrow = nobs, byrow = TRUE)\n\n## Vandermonde matrix A\n\nA = NULL\nfor(i in 0:(nvars - 1)){\n        A = cbind(A, seq(0.5/nobs, 0.5, by = 0.5/nobs)^i)\n}\n\n\nexp.lambdas = exp(-A %*% x.true)\nb = sapply(exp.lambdas, function(x)\n        rexp(1, x))\n\n\n######## use pre-generated data\n#### start of the code\n# nsim = 100\n# train_index = 1 : (nsim * 0.8)\n# validate_index = (nsim * 0.8 + 1) : nsim\n# A = as.matrix(read.csv(\"./testdata/VandermondeMatrix.csv\"))\n# bmat = as.matrix(read.csv(\"./testdata/b_VandermondeMatrix.csv\"))\n# x.true = as.matrix(read.csv(\"./testdata/trueSol.csv\"))\n# x0 = as.matrix(read.csv(\"./testdata//startSol.csv\"))\n# nvars = length(x0)\n# b = bmat[,1]\n\n# params = list(b = bmat[,1],  # The response variables\n#               A = A,  # The matrix\n#               alpha = 1, # 1 means lasso\n#               lambda = 0) # the regularization coefficient\n\n\n\n####### set parameters\nalpha = 0.5\nlasso.lambda = 0.1\nparams = list(\n        b = b,\n        # The response variables\n        A = A,\n        # The matrix\n        alpha = alpha,\n        # 1 means lasso\n        lambda = lasso.lambda\n) # the regularization coefficient\n\n# parameters for PGD\n\n\n### use R implementation of vanilla pgd\n\nfit.pgd = fit_glmnet_fixed_lambda_vanilla_pgd(A, b, alpha = alpha,\n                                              lambda = lasso.lambda)\n\n### use R implementation of vanilla pgd\n\nfit.pgd.cv = fit_glmnet_fixed_lambda_vanilla_pgd_cv(A, b, alpha = alpha,\n                                              lambda = lasso.lambda/10)\n\nfit.pgd.cv$rmse\n\n### use R implementation of vanilla pgd and cross validation\n\nfit.pgd.search.cv = fit_glmnet_search_lambda_vanilla_pgd_cv(A, b, alpha = alpha)\nmin(fit.pgd.search.cv$rmse)\nfit.pgd.search.cv$best.x\nfit.pgd.search.cv$best.idx\nx.true\n\n#\n# ### use bfgs\n# fit.bfgs = optim(rep(0, nvars), ObjFun, params = params, method = \"BFGS\")\n#\n# ### use h2o\n# h2o.init()\n# x.h2o.df = as.h2o(data.frame(b, A))\n# predictors = colnames(x.h2o.df)[-1]\n# response = colnames(x.h2o.df)[1]\n# my.glm.lasso = h2o.glm(\n#         x = predictors,\n#         y = response,\n#         family = 'gamma',\n#         intercept = FALSE,\n#         training_frame = x.h2o.df,\n#         ignore_const_cols = TRUE,\n#         link = \"log\",\n#         lambda_search = TRUE,\n# #        lambda = lasso.lambda,\n#         alpha = alpha,\n#         standardize = TRUE\n# )\n# h2o.shutdown(FALSE)\n# fit.h2o = my.glm.lasso@model$coefficients\n# fit.h2o\n# ### compare results\n# data.frame(\n#         x_true = x.true,\n#         x_prox = fit.pgd,\n#         x_bfgs = fit.bfgs$par,\n#         x_h2o = fit.h2o[-1]\n# )\n\nenet_lambdas = generate_lambda_grid(A, b, alpha)\nmse_matrix = matrix(0, nrow = length(enet_lambdas), ncol = 3)\nh2o.init()\n\ni = 1\nfor(i in 1:length(enet_lambdas)){\n        enet_lambda = enet_lambdas[i]\n        params[[\"lambda\"]] = enet_lambda\n        ### use R implementation of vanilla pgd\n\n        fit.pgd = fit_glmnet_fixed_lambda_vanilla_pgd(A, b, alpha = alpha,\n                                                      lambda = enet_lambda)\n        mse.pgd = sum((predict_exp_glmnet(fit.pgd, params) - b)^2)\n\n        ### use bfgs\n        fit.bfgs = optim(rep(0, nvars), ObjFun, params = params, method = \"BFGS\")$par\n        mse.bfgs = sum((predict_exp_glmnet(fit.bfgs, params) - b)^2)\n\n        ###use h2o\n        x.h2o.df = as.h2o(data.frame(b, A))\n        predictors = colnames(x.h2o.df)[-1]\n        response = colnames(x.h2o.df)[1]\n        my.glm.lasso = h2o.glm(\n                x = predictors,\n                y = response,\n                family = 'gamma',\n                intercept = FALSE,\n                training_frame = x.h2o.df,\n                ignore_const_cols = TRUE,\n                link = \"log\",\n                lambda = enet_lambda,\n                lambda_search = FALSE,\n                alpha = alpha,\n                standardize = FALSE\n        )\n        fit.h2o = my.glm.lasso@model$coefficients\n        mse.h2o = sum((predict_exp_glmnet(fit.h2o, params) - b)^2)\n\n        mse_matrix[i,] = c(mse.pgd, mse.bfgs, mse.pgd)\n}\nmse_matrix\nfit.pgd = fit_glmnet_fixed_lambda_vanilla_pgd(A, b, alpha = alpha,\n                                              lambda = tail(enet_lambdas,1))\n\nfit.bfgs\n# h2o.shutdown(FALSE)\nx.true\nsum((exp(A%*%x.true) - b)^2)\nbest.idx = which.min(mse_matrix[,1])\nfit.pgd = fit_glmnet_fixed_lambda_vanilla_pgd(A, b, alpha = alpha,\n                                    lambda = enet_lambdas[best.idx])\n\nparams.bfgs = params\nparams.bfgs[[\"lambda\"]] = enet_lambdas[best.idx]\nfit.bfgs = optim(rep(0, nvars), ObjFun, params = params.bfgs, method = \"BFGS\")$par\nfit.bfgs\n\nh2o.init()\nx.h2o.df = as.h2o(data.frame(b, A))\npredictors = colnames(x.h2o.df)[-1]\nresponse = colnames(x.h2o.df)[1]\nmy.glm.lasso = h2o.glm(\n        x = predictors,\n        y = response,\n        family = 'gamma',\n        intercept = TRUE,\n        training_frame = x.h2o.df,\n        ignore_const_cols = TRUE,\n        link = \"log\",\n#        lambda = enet_lambda,\n        lambda_search = TRUE,\n        alpha = alpha,\n        standardize = FALSE\n)\nfit.h2o = my.glm.lasso@model$coefficients\nfit.h2o\n### compare results\ndata.frame(\n        x_true = x.true,\n        x_prox = fit.pgd,\n        x_bfgs = fit.bfgs,\n        x_h2o = fit.h2o,\n        x_prox_cv = fit.pgd.search.cv$best.x\n)\nh2o.shutdown(FALSE)\n\n### test cpp NLL of exp\nExpNegativeLogLikelihood(x.true, params)\nExpNegativeLogLikelihood_cpp(x.true, params[[\"A\"]], params[[\"b\"]])\n\n\n### test cpp gradient of NLL of exp\nGradExpNegativeLogLikelihood(x.true, params)\nGradExpNegativeLogLikelihood_cpp(x.true, params[[\"A\"]], params[[\"b\"]])\n\n### test fit glmnet_cpp with fixed lambda\nfit.pgd = fit_glmnet_fixed_lambda_vanilla_pgd(params[[\"A\"]], params[[\"b\"]],\n                                              alpha = params[[\"alpha\"]],\n                                              lambda = enet_lambdas[best.idx])\nfit.pgd\nProxGradDescent_cpp(params[[\"A\"]], params[[\"b\"]],\n                    enet_lambdas[best.idx], params[[\"alpha\"]])\nx.true\n\n\n### test fit glmnet_cpp with lambda searching\nfit.pgd.cpp = fitGlmCv(params[[\"A\"]], params[[\"b\"]],\n         alpha = params[[\"alpha\"]])\n\ndata.frame(\n  x_true = x.true,\n  x_prox = fit.pgd,\n  x_bfgs = fit.bfgs,\n  x_h2o = fit.h2o,\n  x_prox_cv = fit.pgd.search.cv$best.x,\n  x_prox_cv_cpp = fit.pgd.cpp\n)\n\nlibrary(microbenchmark)\n\nnobs = 100\nparams_list = list()\nfor(j in 1:1000){\nA = NULL\nfor(i in 0:(nvars - 1)){\n  A = cbind(A, seq(0.5/nobs, 0.5, by = 0.5/nobs)^i)\n}\n\n\nexp.lambdas = exp(-A %*% x.true)\nb = sapply(exp.lambdas, function(x)\n  rexp(1, x))\n\n####### set parameters\nalpha = 0.5\nlasso.lambda = 0.1\nparams_list[[j]] = list(\n  b = b,\n  # The response variables\n  A = A,\n  # The matrix\n  alpha = alpha,\n  # 1 means lasso\n  lambda = lasso.lambda\n) # the regularization coefficient\n}\nstr(params_list)\n\nr_start = Sys.time()\nr_res_list = lapply(params_list,\n       function(x)\n                fit_glmnet_search_lambda_vanilla_pgd_cv(x[[\"A\"]], x[[\"b\"]])$best.x)\nr_end = Sys.time()\nr_time = r_end - r_start\n\ncpp_start = Sys.time()\ncpp_res_list = lapply(params_list,\n       function(x)\n         fitGlmCv(x[[\"A\"]], x[[\"b\"]], x[[\"alpha\"]]))\ncpp_end = Sys.time()\ncpp_time = cpp_end - cpp_start\nc(r_time, cpp_time)\n\nh2o.init()\n\nh2o_start = Sys.time()\n\nh2o_res_list = lapply(params_list,\n                      function(dat){\n\nb = dat[[\"b\"]]\nA = dat[[\"A\"]]\nalpha = dat[[\"alpha\"]]\nx.h2o.df = as.h2o(data.frame(b, A))\npredictors = colnames(x.h2o.df)[-1]\nresponse = colnames(x.h2o.df)[1]\nmy.glm.lasso = h2o.glm(\n  x = predictors,\n  y = response,\n  family = 'gamma',\n  intercept = TRUE,\n  training_frame = x.h2o.df,\n  ignore_const_cols = TRUE,\n  link = \"log\",\n  #        lambda = enet_lambda,\n  lambda_search = TRUE,\n  alpha = alpha,\n  standardize = FALSE\n)\nreturn(my.glm.lasso@model$coefficients)\n                      }\n)\nh2o_end = Sys.time()\nh2o_time = h2o_end - h2o_start\nh2o_time\ncpp_time\nr_time\nh2o_res_list\ncpp_res_list\nx.true\n\ncompute_mean_vector = function(data_list){\n  tmp_mat = matrix(unlist(data_list), nrow = length(data_list[[1]]))\n  apply(tmp_mat, 1, mean)\n}\nmean_r_res = compute_mean_vector(r_res_list)\nmean_cpp_res = compute_mean_vector(cpp_res_list)\nmean_h2o_res = compute_mean_vector(h2o_res_list)\ndata.frame(x.true = x.true,\n           x.r = mean_r_res,\n           x.h2o = mean_h2o_res,\n           x.cpp = mean_cpp_res)\n\n#### try doing things parallel using foreach\nlibrary(doParallel)\nlibrary(foreach)\n\nforeac\n",
    "created" : 1503507303944.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1820333658",
    "id" : "C1E4F0BA",
    "lastKnownWriteTime" : 1503523936,
    "last_content_update" : 1503523936833,
    "path" : "~/MyCode/GlmNetExpPGD_R/vanilla_prox_descent_search_lambda_cv.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}